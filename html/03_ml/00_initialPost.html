<!DOCTYPE HTML>
<html>
	<head>
		<title>Alberto Castro</title>
		<link rel="icon" type="image/x-icon" href="images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
	</head>
	<body class="is-preload">
        <h3>The 4th Industrial Revolution - Initial Post</h3>
        <p>
            AI has several challenges to tackle, and several are related to <strong>ethics</strong>. According to Leslie (2019), one of the potential harms that AI can cause to customers or users of AI applications is related to biases and discrimination: 
            <br><br>
            AI applications can be biased per the data they were trained with. If the application is trained with data sources from existing biased structures or societies, it can learn and even amplify patterns of discrimination.
            AI applications can learn their designers' beliefs or preferences, leading to poor-biased outcomes.
            <br><br>
            One example of biases in AI applications is described by an article published by The Guardian (2018), where they mention flaws in the AI system Amazon developed to classify resumes. According to them, Amazon has been developing this tool since 2014, but by 2015 Amazon developers realized that their application was not rating candidates neutrally by gender and was biased towards male candidates. The reason is that they used data collected over the last ten years to train their models, data where almost all applicants to jobs in technology were men. This subtle detail made their models prefer males over females when rating new candidates. Amazon claimed that they had updated their models to improve their outcomes and make them fairer, but they ended up shutting their models down when they returned results randomly.
            <br><br>
            Finally, Leslie (2019) states that there is no single way to mitigate biases in AI outcomes. However, he lists different approaches to remediating biased datasets, one of which is the representativeness of data. In the example taken from the Amazon ML models, we notice that males are overrepresented in the dataset, which is not in line with the FAST principles. These principles guide professionals in designing and developing fair, accountable, sustainable, and transparent AI systems (Leslie, 2019).
        </p>
    </body>
</html>

