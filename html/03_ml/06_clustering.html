<!DOCTYPE HTML>
<html>
	<head>
		<title>Alberto Castro</title>
		<link rel="icon" type="image/x-icon" href="images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
	</head>
	<body class="is-preload">
        <h3>Clustering</h3>
		<p>
			Clustering is a technique in data mining and machine learning that involves grouping together similar data points based on their inherent 
			similarities or patterns. It is an unsupervised learning method that seeks to identify natural groupings or clusters within a dataset without 
			any predefined labels or categories.
			<br>
			In clustering, data points that are similar to each other are grouped together, while those that are dissimilar are placed in different groups. 
			The goal is to create meaningful and homogeneous clusters of data points based on their similarities, which can then be used for various purposes, 
			such as data analysis, pattern recognition, anomaly detection, and recommendation systems (Jain, 1999).
		</p>

		<h3>Similarity between data objects</h3>
		To cluster groups in data, we need a way to find similarities among data objects. To find similarity between objects we often use distance. <br>
		Two of the most important distance formulas are:
		<ul>
			<li>Euclidean Distance</li>
			<li>Jaccard Coefficient</li>
		</ul>

		<h4>Euclidean distance</h4>
		The Euclidean distance gives us the distance between two points placed in an Euclidean space (Myrianthous, 2022). Assuming we have two points A and B, 
		and each point has two coordinates x and y, here's the formula to calculate the euclidean distance:
		<code>
			d = âˆš[(xa - xb)^2 + (ya - yb)^2]
		</code>
		Where:
		<ul>
			<li><strong>d</strong> is the distance we want to calculate.</li>
			<li><strong>xa</strong> is the x coordinate of point A.</li>
			<li><strong>xb</strong> is the x coordinate of point B.</li>
			<li><strong>ya</strong> is the y coordinate of point A.</li>
			<li><strong>yb</strong> is the y coordinate of point B.</li>
		</ul>

		<h3>K-Means clustering</h3>
		<p>
			K-means is a popular clustering algorithm in machine learning that aims to partition a dataset into k number of non-overlapping clusters based on 
			the similarity of data points. It is an unsupervised learning method that assigns each data point to one of the k clusters based on the distance 
			between the data point and the centroid of each cluster.<br><br>
			The k-means algorithm starts with randomly initializing k centroids, which are the center points of the initial clusters. Then, it iteratively assigns each 
			data point to the nearest centroid and updates the centroids by taking the mean of the data points within each cluster. This process is repeated until 
			convergence, which is typically determined by a predefined number of iterations or when the centroids do not change significantly.
			<br><br>
			It is sensitive to the initial placement of centroids and can sometimes converge to suboptimal solutions. Therefore, it is common to run the k-means algorithm 
			multiple times with different initializations and choose the best clustering result based on a predefined evaluation metric.
		</p>

		<h4>The elbow method</h4>

		<p>
			The elbow method is a graphical approach used in clustering to determine the optimal number of clusters (k) to use in a k-means clustering 
			algorithm. It involves plotting the variance or the sum of squared distances (SSD) of the data points within each cluster against the number of 
			clusters (k), and identifying the "elbow point" on the plot where the variance or SSD starts to level off (Ketchen, 1996).<br><br>

			The intuition behind the elbow method is that as the number of clusters increases, the variance or SSD within each cluster tends to decrease, 
			as each data point is assigned to a more specific cluster. However, adding too many clusters can result in overfitting, where each cluster becomes 
			too small and the clustering solution becomes overly complex. On the other hand, if the number of clusters is too low, the variance or SSD within 
			each cluster may be high, resulting in poor clustering performance.<br><br>

			By looking for the elbow point on the plot of variance or SSD versus the number of clusters, the elbow method helps to identify a reasonable value 
			for k, where the decrease in variance or SSD levels off and adding more clusters does not significantly improve the clustering performance. The 
			elbow point is often considered as the optimal number of clusters to use in the k-means algorithm for a given dataset.
		</p>
		<h4>The elbow method in python</h4>

		The following example was taken from the Unit 6 assignment. In this assignment we tried to find patterns in Airbnb data from New Yorck City. In this case, 
		we are only considering properties in Manhattan and entire home or apartments as room type. <br>
		Having a data frame with locations (latitude and longitude) and prices, we want to cluster those locations by price (Castro et. al., 2023).<br>
		<br>

		<figure>
			<code>
				<span class="comment"># manhattan df</span><br>
				mdf = df[df[<span class="string">'neighbourhood_group_Manhattan'</span>] == 1]<br>
				mdf = mdf[mdf[<span class="string">'room_type_Entire home/apt'</span>] == 1]<br>
				mdf = mdf[[<span class="string">'price'</span>, <span class="string">'longitude'</span>, <span class="string">'latitude'</span>]]<br>
				<br>
				data = list(zip(mdf[<span class="string">'latitude'</span>], mdf[<span class="string">'longitude'</span>]))<br>
				max_k = len(data)<br>
				scores = []<br>
				<br>
				for i in range(1, 20):<br>
				&nbsp;&nbsp;&nbsp;&nbsp;kmeans = KMeans(n_clusters=i)<br>
				&nbsp;&nbsp;&nbsp;&nbsp;kmeans.fit(data)<br>
				&nbsp;&nbsp;&nbsp;&nbsp;scores.append(kmeans.inertia_)<br>
					<br>
				plt.plot(range(1, 20), scores, marker=<span class="string">'o'</span>)<br>
				plt.title(<span class="string">'Elbow method'</span>)<br>
				plt.xlabel(<span class="string">'Number of clusters'</span>)<br>
				plt.ylabel(<span class="string">'Score'</span>)<br>
			</code>
			<figcaption class="image-caption">Code use the elbow technique. Code taken from the team assignment in Unit 6 (Castro et. al., 2023).</figcaption>
        </figure>

		<figure>
			<img src="../../images/ML/images/elobow.png" alt="The elbow technique"/>
			<figcaption class="image-caption">The elbow techinque (Castro et. al., 2023).</figcaption>
		</figure>

		In the plot above, we observe that k = 2 is a proper number of clusters.<br><br>

		<h4>K-Means in python</h4>

		Once we have the proper number of clusters using the elbow method, let's review how to do that in Python. We will use the code submitted in Unit 6 (Castro et. al., 2023).

		<br><br>
		<figure>
			<code class="my-code">
				kmeans = KMeans(n_clusters=2)<br>
				ndata = mdf.copy()<br>
				ndata[<span class="string">"cluster"</span>] = kmeans.fit_predict(ndata[[<span class="string">"latitude"</span>, <span class="string">"longitude"</span>, <span class="string">"price"</span>]])<br>
				centroids = kmeans.cluster_centers_<br>
				labels = kmeans.labels_<br>
				colors = [<span class="string">"g"</span>, <span class="string">"r"</span>,]<br>
				<br>
				plt.scatter(<br>
				&nbsp;&nbsp;&nbsp;&nbsp;ndata[<span class="string">"latitude"</span>],<br>
				&nbsp;&nbsp;&nbsp;&nbsp;ndata[<span class="string">"longitude"</span>],<br>
				&nbsp;&nbsp;&nbsp;&nbsp;ndata[<span class="string">"price"</span>],<br>
				&nbsp;&nbsp;&nbsp;&nbsp;color=[colors[l_] for l_ in labels],<br>
				&nbsp;&nbsp;&nbsp;&nbsp;label=labels,<br>
				&nbsp;&nbsp;&nbsp;&nbsp;alpha = 0.1<br>
				)<br>
				plt.scatter(<br>
				&nbsp;&nbsp;&nbsp;&nbsp;centroids[:, 0],<br>
				&nbsp;&nbsp;&nbsp;&nbsp;centroids[:, 1],<br>
				&nbsp;&nbsp;&nbsp;&nbsp;color=<span class="string">"y"</span>,<br>
				&nbsp;&nbsp;&nbsp;&nbsp;marker = <span class="string">"x"</span>,<br>
				&nbsp;&nbsp;&nbsp;&nbsp;s=500,<br>
				&nbsp;&nbsp;&nbsp;&nbsp;linewidths = 3,<br>
				&nbsp;&nbsp;&nbsp;&nbsp;zorder = 10<br>
				)<br>
				ndata[[<span class="string">"latitude"</span>, <span class="string">"longitude"</span>, <span class="string">"price"</span>, <span class="string">"cluster"</span>]].head(20)
			</code>
			Output:
			<table>
				<tr>
				  <th>latitude</th>
				  <th>longitude</th>
				  <th>price</th>
				  <th>cluster</th>
				</tr>
				<tr>
				  <td>40.75362</td>
				  <td>-73.98377</td>
				  <td>225</td>
				  <td>1</td>
				</tr>
				<tr>
				  <td>40.79851</td>
				  <td>-73.94399</td>
				  <td>80</td>
				  <td>0</td>
				</tr>
				<tr>
				  <td>40.74767</td>
				  <td>-73.97500</td>
				  <td>200</td>
				  <td>1</td>
				</tr>
				<tr>
				  <td>40.71344</td>
				  <td>-73.99037</td>
				  <td>150</td>
				  <td>0</td>
				</tr>
				<tr>
				  <td>40.80316</td>
				  <td>-73.96545</td>
				  <td>135</td>
				  <td>0</td>
				</tr>
				<tr>
				  <td>40.73530</td>
				  <td>-74.00525</td>
				  <td>120</td>
				  <td>0</td>
				</tr>
				<tr>
				  <td>40.79685</td>
				  <td>-73.94872</td>
				  <td>190</td>
				  <td>1</td>
				</tr>
				<tr>
				  <td>40.76715</td>
				  <td>-73.98533</td>
				  <td>150</td>
				  <td>0</td>
				</tr>
				<tr>
				  <td>40.72920</td>
				  <td>-73.98542</td>
				  <td>180</td>
				  <td>0</td>
				</tr>
				<tr>
				  <td>40.81175</td>
				  <td>-73.94478</td>
				  <td>150</td>
				  <td>0</td>
				</tr>
			</table>
			<figcaption class="image-caption">Code to cluster locations by price. Code taken from the team assignment in Unit 6 (Castro et. al., 2023).</figcaption>
        </figure>

		<figure>
			<img src="../../images/ML/images/clustering.png" alt="Clustered locations by price"/>
			<figcaption class="image-caption">Clustered locations by price (Castro et. al., 2023).</figcaption>
		</figure>

		The two clusters are:
		<ul>
			<li>Red (Cluster 0): expensive areas.</li>
			<li>Green (Cluster 1): low-priced areas.</li>
		</ul>

		We can observe i the figure below that the most expensive areas are located near touristic places such as the Empire State, 
		the Brooklyn Brigde, the Statue of Liberty, Central Park, among others (Castro et. al., 2023).

		<figure>
			<img src="../../images/ML/images/clusteringOnMap.png" alt="Clustered locations by price"/>
			<figcaption class="image-caption">Clustered locations and touristic places shown in the map (Castro et. al., 2023).</figcaption>
		</figure>
    </body>
</html>

