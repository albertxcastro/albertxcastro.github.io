<!DOCTYPE HTML>
<html>
	<head>
		<title>Alberto Castro</title>
		<link rel="icon" type="image/x-icon" href="images/favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<meta content="text/html; charset=UTF-8" http-equiv="content-type">
		<style type="text/css">
			.lst-kix_nkoltsy5db7q-2>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-2
			}

			.lst-kix_nkoltsy5db7q-6>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-6, decimal) ". "
			}

			.lst-kix_nkoltsy5db7q-7>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-7, lower-latin) ". "
			}

			.lst-kix_nkoltsy5db7q-2>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-2, lower-roman) ". "
			}

			ol.lst-kix_nkoltsy5db7q-3.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-3 0
			}

			.lst-kix_nkoltsy5db7q-8>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-8
			}

			.lst-kix_nkoltsy5db7q-0>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-0, decimal) ". "
			}

			.lst-kix_nkoltsy5db7q-1>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-1, lower-latin) ". "
			}

			.lst-kix_nkoltsy5db7q-5>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-5
			}

			.lst-kix_nkoltsy5db7q-8>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-8, lower-roman) ". "
			}

			ol.lst-kix_nkoltsy5db7q-1.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-1 0
			}

			ol.lst-kix_nkoltsy5db7q-7.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-7 0
			}

			ol.lst-kix_nkoltsy5db7q-0 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-1 {
				list-style-type: none
			}

			.lst-kix_nkoltsy5db7q-7>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-7
			}

			ol.lst-kix_nkoltsy5db7q-4 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-5 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-2 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-3 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-8 {
				list-style-type: none
			}

			.lst-kix_nkoltsy5db7q-1>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-1
			}

			.lst-kix_nkoltsy5db7q-4>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-4
			}

			ol.lst-kix_nkoltsy5db7q-6 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-7 {
				list-style-type: none
			}

			ol.lst-kix_nkoltsy5db7q-4.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-4 0
			}

			.lst-kix_nkoltsy5db7q-3>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-3
			}

			ol.lst-kix_nkoltsy5db7q-6.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-6 0
			}

			.lst-kix_nkoltsy5db7q-0>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-0
			}

			ol.lst-kix_nkoltsy5db7q-2.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-2 0
			}

			li.li-bullet-0:before {
				margin-left: -18pt;
				white-space: nowrap;
				display: inline-block;
				min-width: 18pt
			}

			ol.lst-kix_nkoltsy5db7q-8.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-8 0
			}

			.lst-kix_nkoltsy5db7q-6>li {
				counter-increment: lst-ctn-kix_nkoltsy5db7q-6
			}

			.lst-kix_nkoltsy5db7q-3>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-3, decimal) ". "
			}

			ol.lst-kix_nkoltsy5db7q-5.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-5 0
			}

			.lst-kix_nkoltsy5db7q-4>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-4, lower-latin) ". "
			}

			.lst-kix_nkoltsy5db7q-5>li:before {
				content: "" counter(lst-ctn-kix_nkoltsy5db7q-5, lower-roman) ". "
			}

			ol.lst-kix_nkoltsy5db7q-0.start {
				counter-reset: lst-ctn-kix_nkoltsy5db7q-0 0
			}

			ol {
				margin: 0;
				padding: 0
			}

			table td,
			table th {
				padding: 0
			}

			.c9 {
				border-right-style: solid;
				border-top-width: 0pt;
				border-right-width: 0pt;
				padding-left: 0pt;
				border-left-width: 0pt;
				border-top-style: solid;
				border-left-style: solid;
				border-bottom-width: 0pt;
				border-bottom-style: solid;
				padding-right: 0pt
			}

			.c1 {
				padding-top: 0pt;
				padding-bottom: 0pt;
				line-height: 1.15;
				orphans: 2;
				widows: 2;
				text-align: left;
				height: 11pt
			}

			.c2 {
				color: #000000;
				font-weight: 400;
				text-decoration: none;
				vertical-align: baseline;
				font-size: 12pt;
				font-family: "Arial";
				font-style: normal
			}

			.c4 {
				color: #000000;
				font-weight: 400;
				text-decoration: none;
				vertical-align: baseline;
				font-size: 11pt;
				font-family: "Arial";
				font-style: normal
			}

			.c14 {
				color: #000000;
				font-weight: 700;
				text-decoration: none;
				vertical-align: baseline;
				font-family: "Arial";
				font-style: normal
			}

			.c13 {
				color: #222222;
				font-weight: 400;
				text-decoration: none;
				vertical-align: baseline;
				font-family: "Arial";
				font-style: normal
			}

			.c11 {
				padding-top: -6pt;
				padding-bottom: 0pt;
				line-height: 1.15;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			.c8 {
				padding-top: 0pt;
				padding-bottom: 0pt;
				line-height: 1.15;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			.c3 {
				padding-top: 0pt;
				padding-bottom: 0pt;
				line-height: 1.15;
				orphans: 2;
				widows: 2;
				text-align: justify
			}

			.c18 {
				font-weight: 400;
				text-decoration: none;
				vertical-align: baseline;
				font-family: "Arial";
				font-style: normal
			}

			.c10 {
				background-color: #ffffff;
			}

			.c16 {
				padding: 0;
				margin: 0
			}

			.c12 {
				color: inherit;
				text-decoration: inherit
			}

			.c5 {
				color: #374151;
				font-size: 12pt
			}

			.c19 {
				margin-left: 36pt;
				padding-left: 0pt
			}

			.c0 {
				color: #1155cc;
				font-size: 12pt
			}

			.c15 {
				font-style: italic
			}

			.c6 {
				height: 11pt
			}

			.c17 {
				color: #222222
			}

			.c7 {
				font-size: 12pt
			}

			.title {
				padding-top: 0pt;
				color: #000000;
				font-size: 26pt;
				padding-bottom: 3pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			.subtitle {
				padding-top: 0pt;
				color: #666666;
				font-size: 15pt;
				padding-bottom: 16pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			li {
				color: #000000;
				font-size: 11pt;
				font-family: "Arial"
			}

			p {
				margin: 0;
				color: #000000;
				font-size: 11pt;
				font-family: "Arial"
			}

			h1 {
				padding-top: 20pt;
				color: #000000;
				font-size: 20pt;
				padding-bottom: 6pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			h2 {
				padding-top: 18pt;
				color: #000000;
				font-size: 16pt;
				padding-bottom: 6pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			h4 {
				padding-top: 14pt;
				color: #666666;
				font-size: 12pt;
				padding-bottom: 4pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			h5 {
				padding-top: 12pt;
				color: #666666;
				font-size: 11pt;
				padding-bottom: 4pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			h6 {
				padding-top: 12pt;
				color: #666666;
				font-size: 11pt;
				padding-bottom: 4pt;
				font-family: "Arial";
				line-height: 1.15;
				page-break-after: avoid;
				font-style: italic;
				orphans: 2;
				widows: 2;
				text-align: left
			}

			.rectangle {
				width: 950px; /* Width of the rectangle */
				height: 3800px; /* Height of the rectangle */
				border: 2px solid black; /* Border thickness and color */
				background: none; /* Ensures the rectangle is not filled */
				border-color: #e5e5e5;
				padding: 50px;
			}
		</style>
	</head>
	<body class="is-preload">
        <h3>Literature review</h3>
		<div class="c10 doc-content rectangle">
		<p class="c3"><span class="c14 c7">How significantly do emoji-based contextual signals improve the precision of
            sentiment analysis models in identifying sarcasm within text-based online communications?</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c14 c7">Introduction</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">In in-person communication, facial expressions offer insight into the message&#39;s
            emotional tone and underlying sentiment, which helps better understand its meaning (Boutet et al., 2021).
            However, in the context of online interactions, facial expressions were first replaced by emoticons (emotion
            + icon: a combination of punctuation marks and other characters to create facial expressions in plain text
            messages (Oxford reference, 2024)) and later by the usage of emojis (pictorial characters that denote
            different emotions, places, or things (McArthur et al., 2018)).</span></p>
		<p class="c3"><span class="c2">Emojis have become integral to our daily communication, especially in online
            conversations. With the rapid growth of social media and messaging platforms, emojis have become popular for
            expressing emotions and sentiments in text-based communication. As a result, emoji-based sentiment analysis
            has become an important topic.</span></p>
		<p class="c3"><span class="c2">This literature review aims to explore the different approaches and techniques used
            for incorporating emojis in sentiment analysis and to evaluate the effectiveness of these approaches in
            accurately capturing the sentiment of the text, focusing on detecting sarcasm in text messages. By examining
            the existing literature on this topic, we hope to understand better the role of emojis in detecting sarcasm
            in sentiment analysis.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c14 c7">Literature Review</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Several authors have researched sarcasm detection using different techniques. The
            most used techniques are rule-based and machine learning-based (Kumar et al., 2017).</span></p>
		<p class="c3"><span class="c2">Rule-based natural language processing is an old approach that relies on rules
            created by expert humans to categorize the text under analysis. The limitation of this approach is that if
            the rule does not exist, the text is not categorized, which results in poor analysis performance
            (Goodey).</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">For the reason above, we will focus on machine learning approaches.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Liebrecht et al.(2013) conducted one of the most exciting experiments. They collected
            two sets of tweets: the first had 77,948 tweets posted in 2010 and explicitly marked as sarcastic by users,
            using the hashtag #sarcasme (the word sarcasm in Dutch). The second set comprised tweets posted on February
            1, 2013, containing approximately 3.3 million tweets. The authors used a Balance Winnow model to classify
            sarcastic tweets in the experiment. Their results are fascinating because of their model&#39;s metrics
            results and some insights they obtained from the data. Their classification was overall good. They used the
            TPR and AUC metrics to measure their model&#39;s performance. From two tests they did on a balanced and an
            unbalanced dataset, their highest True Positive Rate was 79%. However, they also discovered insights about
            the tweets: most sarcastic tweets had specific topics, like school topics (exams, professors, classes), the
            weather, celebrities, and TV shows, among others. Additionally, they found solid linguistic markers of
            sarcasm: the words joke, lol (which means lots of laughs and usually changes the meaning of the message),
            humor, cynicism, and irony. These findings are very beneficial for our research.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">On the other hand, Ghosh et al. (2015) propose a similar methodology for sarcasm
            detection. In their paper, they call the task to identify a tweet as sarcastic or literal Literal/Sarcastic
            Sense Disambiguation (LSSD). The authors start by addressing two main problems.</span></p>
		<ol class="c16 lst-kix_nkoltsy5db7q-0 start" start="1">
			<li class="c3 c19 li-bullet-0"><span class="c2">How to get relevant data: gathering a dataset of words that may
                have a literal or sarcastic meaning based on the context.</span></li>
			<li class="c3 c19 li-bullet-0"><span class="c2">Given a statement containing a word from the dataset defined in
                the point above, how to determine that a word is being utilized in its actual, literal meaning or a
                sarcastic meaning?</span></li>
		</ol>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">The authors used an Amazon service called Amazon Mechanical Turk to solve the first
            point. In this crowdsourcing service, humans can be hired remotely to complete tasks that computers cannot
            perform. In this case, they were given specific instructions: to rephrase sarcastic messages and create a
            new message with literal meaning. This approach is intelligent, given the nature of the data, composed of
            human-written tweets.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">To address the second point, the authors compared two approaches for classifying
            tweets: Distributional approaches, based on the fact that words that appear and are employed in similar
            contexts often convey similar meanings (Ghosh et al., 2015), and classification approaches, where each
            statement is classified as sarcasm or literal meaning.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">An important point to note here is the similarity of gathering the first collection
            of data: both Liebrecht et al.(2013) and (Ghosh et al., 2015) choose the same approach of gathering tweets
            that contain the hashtag #sarcasm, but in both papers, the dataset is processed using different techniques.
        </span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">The approach Liebrecht et al.(2013) and Ghosh et al. (2015) utilized for building
            their embeddings are similar. Both papers use n-grams as word embeddings, where all words are converted to
            lowercase, except for words written in capital letters, since they signal sarcasm (Liebrecht et al., 2013).
            However, it is essential to note that Ghosh et al. (2015) also experimented with other word embeddings, like
            Weighted Textual Matrix Factorization, Continuous Bag-of-Words (CBOW), and GloVe Representation.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">On the other hand, they use different classification models: Liebrecht et al.(2013)
            use a Winnow classifier suitable for binary classification, while Ghosh et al. (2015) employ a baseline SVM
            model and variations for each type of word embedding.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Ghosh et al. (2015) show how their SVM model that uses CBOW outperforms other models
            on smaller datasets. The study also acknowledged the potential issue of noisy data using hashtags as gold
            labels and plans to conduct further experiments to address this. </span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Both studies have shown interesting approaches to classifying sarcastic text.
            However, both use the same approach of using hashtags (#sarcasm or #sarcasme) as golden labels, and at the
            same time, both agree that that might not be the best choice since there is no guarantee that tweets using
            those hashtags will be indeed sarcastic.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">The above situation drives us to explore other discriminatory elements, such as
            emojis, within the text we want to classify.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">In social media, emojis are crucial, as highlighted in a study where they were used
            as a critical feature to identify users with depression. This research found emojis to be among the most
            significant factors in modeling. Consequently, effective representation and interpretation of emojis is
            essential (Barry et al., 2022).</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Barbieri et al. (2016) conducted quantitative and qualitative evaluations to test the
            effectiveness of their models. For the quantitative analysis, they compared the model&#39;s output with
            human assessments of emoji semantics. The qualitative evaluation involved visualizing emoji vectors to
            observe clustering patterns and determine the association between emojis and text tokens. However, they
            observed that reducing the dimensionality of their set using t-Distributed Stochastic Neighbor Embedding
            (t-SNE) introduced noise, causing some emojis to be out of context. Their work is significant in
            understanding how emojis contribute to text enrichment in social media. They mention that it would be
            interesting to see how different cultures use emojis and how emojis evolve. It is an interesting statement
            that Barry et al. (2022) explain later in their research when they state that emojis represent emotional
            nuances and social or political contexts.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Chen et al. (2018) proposed bi-sense emoji embeddings to represent each emoji with
            two distinct vectors: positive and negative. This approach acknowledges that emojis can have different
            meanings in different sentimental contexts. In this case, tweets were collected and processed, with emojis
            considered special words. Sentiments were initially labeled using the Vader sentiment analysis
            algorithm.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Chen et al. (2018) state that bi-sense emoji embeddings provide a more nuanced and
            effective representation of the complex semantics associated with emojis. They capture different emotional
            contexts in which an emoji might be used, enhancing the sentiment analysis model&#39;s ability to interpret
            and classify sentiments accurately. In the context of sarcasm detection, emojis can be beneficial since they
            can provide extra context that allows us to know when a message is not literal: if the sentiment of an emoji
            contradicts the sentiment of the message, it might be a signal of sarcasm.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Chen et al.&#39;s (2018) approach uses long short-term memory (LSTM) networks
            integrated with pre-trained emoji embeddings. They propose Two mechanisms: Word-guide Attention-based LSTM
            and Multi-level Attention-based LSTM, using bi-sense emoji embeddings. Their results proved that the
            bi-sense emoji embeddings improved performance in sentiment analysis tasks compared to state-of-the-art
            models.</span></p>
		<p class="c3"><span class="c2">Moreover, models with bi-sense emoji embeddings outperformed baseline models that did
            not use emoji features or used conventional word-emoji embeddings.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Additionally, the attention-based LSTM models, particularly those using the bi-sense
            embeddings, demonstrated the capability to focus on the most sentimentally relevant words and emojis in a
            given context, leading to more accurate sentiment classification.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">In another study, Eisner et al. (2016) point out that natural language processing and
            sentiment analysis currently rely on tools like word2vec or GloVe for word embedding. However, these tools
            do not include emoji representations, which, according to Barry et al. (2022), are crucial in accurately
            representing and understanding user behavior on social media platforms through natural language processing.
            For this reason, Eisner et al. (2016) released a robust emoji embedding tool called emoji2vec, which embeds
            emoji Unicode symbols. </span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Eisner et al. (2016) show how their new emoji embedding tool outperforms previous
            models in sentiment analysis tasks, proving that including emojis in natural language processing models is
            crucial in modern online content.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Their findings indicate that enhancing word2vec with emoji embeddings boosts the
            overall accuracy in classifying the entire corpus and significantly enhances the performance of tweets
            containing emojis. This implies that incorporating emoji embeddings might enhance results in other social
            natural language processing tasks (like sarcasm detection). Additionally, their analysis reveals that
            emoji2vec tends to surpass the emoji embeddings developed by Barbieri et al. (2016) in terms of performance
            despite being trained on a smaller dataset with a simpler model.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Barry et al. (2022) followed up on the work Eisner et al. (2016) performed on using
            emojis in natural language processing. They outline the importance of emojis in sentiment analysis and the
            importance of building emoji datasets that are easy to maintain. </span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Barry et al. (2022) found a way to embed emojis into sentiment analysis models by
            creating a map of emojis and a set of words representing the emotions they evoke, allowing emojis to have a
            wide range of emotions represented as text. They did this by scrapping data using a Python library called
            Beautiful Soup. They scrapped different dictionaries that matched each emoji&#39;s description, resulting in
            more than ten thousand descriptions for all 1816 emojis taken from the latest list from Unicode.org as of
            2021. Nevertheless, they decided not to use the different skin tones available in the emoji list since it
            would introduce complexity, because the number of emojis is multiplied by 5x, and also because there is
            evidence that shows that skin tones do not modify or alter sentiments in the emoji itself.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Barry et al. (2022) tested their new emoji embeddings on the same baseline models
            utilized by Eisner et al. (2016). They demonstrated that their new emoji embeddings outperformed the
            baseline models using the most frequently used emojis, especially effective using the random forests
            algorithm.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c14 c7">Conclusions</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">The literature review presented in this document aims to integrate two areas of study
            of natural language processing: sarcasm detection using plain text, especially with hashtag analysis, and
            the field of emoji-based natural language processing. We aim to demonstrate how combining these methods can
            improve the accuracy of sarcasm detection in social media content, like Twitter posts.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Some approaches for sarcasm detection focused on plain text, using hashtags like
            #sarcasm. This method, despite its effectiveness, has limitations. Sarcasm involves complexities beyond just
            messages in plain text, including tone and implied meanings, which are not fully captured by text and
            hashtags alone.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Emojis, however, add emotional and contextual cues to communication. They substitute
            for non-verbal signals and expressions crucial in understanding sarcasm in real-life interactions. Moreover,
            in social media, where messages are brief, emojis offer insights into the emotional tones of
            messages.</span></p>
		<p class="c3 c6"><span class="c2"></span></p>
		<p class="c3"><span class="c2">Finally, Incorporating emoji analysis into sarcasm detection allows for a deeper
            comprehension of online communication. Emojis can indicate sarcasm, either supporting or contradicting the
            text. This dual text and visual cues approach can lead to better sarcasm interpretation.</span></p>
		<p class="c3 c6"><span class="c4"></span></p>
		<p class="c3"><span class="c7 c14">References</span></p>
		<p class="c3 c6"><span class="c14 c7"></span></p>
		<p class="c11"><span class="c7">Barbieri, F., Ronzano, F. and Saggion, H., 2016. What does this Emoji Mean? A Vector
            Space Skip-Gram Model for Twitter Emojis. In: N. Calzolari, K. Choukri, T. Declerck, S. Goggi, M. Grobelnik,
            B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk and S. Piperidis, eds. Proceedings of the Tenth
            International Conference on Language Resources and Evaluation (LREC&#39;16), May 2016, Portoro&#382;,
            Slovenia. European Language Resources Association (ELRA), pp.3967-3972. Available at:</span><span
				class="c7"><a class="c12"
							  href="https://www.google.com/url?q=https://aclanthology.org/L16-1626&amp;sa=D&amp;source=editors&amp;ust=1707687633395719&amp;usg=AOvVaw1YCLdVJebZqvxjTQRqwSSv">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://aclanthology.org/L16-1626&amp;sa=D&amp;source=editors&amp;ust=1707687633395983&amp;usg=AOvVaw1mSS4mnnnhaV7tldC9m4Wg">https://aclanthology.org/L16-1626</a></span><span
				class="c2">&nbsp;[Accessed 14 Jan. 2024].</span></p>
		<p class="c11 c6"><span class="c2"></span></p>
		<p class="c8"><span class="c7">Boutet, I., LeBlanc, M., Chamberland, J.A. and Collin, C.A., 2021. Emojis influence
            emotional communication, social attributions, and information processing. </span><span
				class="c15 c7">Computers in Human Behavior</span><span class="c7">, 119, pp.106722. Available
            at:</span><span class="c7"><a class="c12"
										  href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000443&amp;sa=D&amp;source=editors&amp;ust=1707687633396284&amp;usg=AOvVaw3HXtwaxc15-BSOltRoaEFS">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000443&amp;sa=D&amp;source=editors&amp;ust=1707687633396409&amp;usg=AOvVaw2GEpXmfPkWTa4-e0mczilo">https://www.sciencedirect.com/science/article/pii/S0747563221000443</a></span><span
				class="c5">&nbsp;</span><span class="c7">[Accessed 14 Jan. 2024]. DOI:</span><span class="c5"><a class="c12"
																												 href="https://www.google.com/url?q=https://doi.org/10.1016/j.chb.2021.106722&amp;sa=D&amp;source=editors&amp;ust=1707687633396542&amp;usg=AOvVaw0iI9tnzmNNmTWemtfJj2cH">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://doi.org/10.1016/j.chb.2021.106722&amp;sa=D&amp;source=editors&amp;ust=1707687633396653&amp;usg=AOvVaw3D8070qY8SvwCcho5DCtHD">https://doi.org/10.1016/j.chb.2021.106722</a></span><span
				class="c5">.</span></p>
		<p class="c11 c6"><span class="c5 c18"></span></p>
		<p class="c11"><span class="c2">Chen, Y., Yuan, J., You, Q., Luo, J., 2018. Twitter sentiment analysis via bi-sense
            emoji embedding and attention-based LSTM. In: ICM, pp. 117&ndash;125.</span></p>
		<p class="c11 c6"><span class="c2"></span></p>
		<p class="c9 c8"><span class="c7">Eisner, B., Rockt&auml;schel, T., Augenstein, I., Bo&scaron;njak, M. and Riedel,
            S., 2016. Emoji2vec: Learning emoji representations from their description. In: Workshop on NLP for Social
            Media, pp.48-54. Austin, TX, USA: Association for Computational Linguistics. Available at:</span><span
				class="c5"><a class="c12"
							  href="https://www.google.com/url?q=https://www.aclweb.org/anthology/W16-6208&amp;sa=D&amp;source=editors&amp;ust=1707687633396989&amp;usg=AOvVaw2i7ZNRaXUiBus2JsJ1LxMg">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://www.aclweb.org/anthology/W16-6208&amp;sa=D&amp;source=editors&amp;ust=1707687633397090&amp;usg=AOvVaw3JlE0_xuvM-EMjeugvM8Zt">https://www.aclweb.org/anthology/W16-6208</a></span><span
				class="c5">&nbsp;</span><span class="c2">[Accessed 14 Jan. 2024].</span></p>
		<p class="c8 c9"><span class="c7">Ghosh, D., Guo, W. and Muresan, S., 2015. Sarcastic or Not: Word Embeddings to
            Predict the Literal or Sarcastic Meaning of Words. In: </span><span class="c7 c15">Conference on Empirical
            Methods in Natural Language Processing</span><span class="c7">. Available at:</span><span class="c7"><a
				class="c12"
				href="https://www.google.com/url?q=https://api.semanticscholar.org/CorpusID:6202343&amp;sa=D&amp;source=editors&amp;ust=1707687633397318&amp;usg=AOvVaw1-bfLyMgdeAYWZyZfyYXd-">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://api.semanticscholar.org/CorpusID:6202343&amp;sa=D&amp;source=editors&amp;ust=1707687633397436&amp;usg=AOvVaw3l6yWGmtnaHWOY1iSgErKR">https://api.semanticscholar.org/CorpusID:6202343</a></span><span
				class="c2">&nbsp;[Accessed 14 Jan. 2024].</span></p>
		<p class="c11"><span class="c7">Goodey, B., n.d. Machine Learning &amp; NLP. [online] SentiSum. Available
            at:</span><span class="c7"><a class="c12"
										  href="https://www.google.com/url?q=https://www.sentisum.com/success-article/machine-learning-nlp&amp;sa=D&amp;source=editors&amp;ust=1707687633397643&amp;usg=AOvVaw3uDp9BxI0nz4y4UYYC7aS0">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://www.sentisum.com/success-article/machine-learning-nlp&amp;sa=D&amp;source=editors&amp;ust=1707687633397756&amp;usg=AOvVaw1r0aVC-PxdFz-mdpx38GlI">https://www.sentisum.com/success-article/machine-learning-nlp</a></span><span
				class="c5">&nbsp;</span><span class="c2">[Accessed 14 Jan. 2024].</span></p>
		<p class="c6 c11"><span class="c2"></span></p>
		<p class="c11"><span class="c17 c7">Kumar, L., Somani, A. and Bhattacharyya, P., 2017. Approaches for computational
            sarcasm detection: A survey. </span><span class="c15 c17 c7">ACM CSUR</span><span class="c7 c17">,
        </span><span class="c15 c17 c7">27</span><span class="c13 c7">.</span></p>
		<p class="c11 c6"><span class="c7 c13"></span></p>
		<p class="c9 c8"><span class="c7">Liebrecht, C., Kunneman, F. and Van den Bosch, A., 2013. The perfect solution for
            detecting sarcasm in tweets #not. In: A. Balahur, E. van der Goot and A. Montoyo, eds. Proceedings of the
            4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. Atlanta,
            Georgia: Association for Computational Linguistics, pp.29-37. Available at:</span><span class="c7"><a
				class="c12"
				href="https://www.google.com/url?q=https://aclanthology.org/W13-1605&amp;sa=D&amp;source=editors&amp;ust=1707687633398151&amp;usg=AOvVaw2czBffgmdecXkjyLTSBzsF">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://aclanthology.org/W13-1605&amp;sa=D&amp;source=editors&amp;ust=1707687633398250&amp;usg=AOvVaw0t40HG9Kau6ERdzb00nMyY">https://aclanthology.org/W13-1605</a></span><span
				class="c2">&nbsp;[Accessed 14 Jan. 2024].</span></p>
		<p class="c3 c6"><span class="c14 c7"></span></p>
		<p class="c8"><span class="c7">McArthur, T., Lam-McArthur, J., and Fontaine, L. (Eds.), 2018. Emoji. In:
        </span><span class="c15 c7">The Oxford Companion to the English Language</span><span class="c7">. Oxford
            University Press. Available at:</span><span class="c7"><a class="c12"
																	  href="https://www.google.com/url?q=https://www.oxfordreference.com/view/10.1093/acref/9780199661282.001.0001/acref-9780199661282-e-1386&amp;sa=D&amp;source=editors&amp;ust=1707687633398545&amp;usg=AOvVaw28uwlcBuEDzWglG2ZP9FmG">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://www.oxfordreference.com/view/10.1093/acref/9780199661282.001.0001/acref-9780199661282-e-1386&amp;sa=D&amp;source=editors&amp;ust=1707687633398680&amp;usg=AOvVaw030nxYXacZNOrGvcpx_3tk">https://www.oxfordreference.com/view/10.1093/acref/9780199661282.001.0001/acref-9780199661282-e-1386</a></span><span
				class="c5">&nbsp;</span><span class="c7">[Accessed 14 Jan. 2024].</span></p>
		<p class="c3 c6"><span class="c14 c7"></span></p>
		<p class="c8"><span class="c7">Oxford Reference, 2024. Emoticon. [online] Available at:</span><span class="c7"><a
				class="c12"
				href="https://www.google.com/url?q=https://www.oxfordreference.com/view/10.1093/oi/authority.20110803095749926&amp;sa=D&amp;source=editors&amp;ust=1707687633398979&amp;usg=AOvVaw3bMXny1rBpkOOyrRaSE6hV">&nbsp;</a></span><span
				class="c0"><a class="c12"
							  href="https://www.google.com/url?q=https://www.oxfordreference.com/view/10.1093/oi/authority.20110803095749926&amp;sa=D&amp;source=editors&amp;ust=1707687633399103&amp;usg=AOvVaw1GfAYczWrAffMByCGCkl_q">https://www.oxfordreference.com/view/10.1093/oi/authority.20110803095749926</a></span><span
				class="c5">&nbsp;</span><span class="c2">[Accessed 14 Jan. 2024].</span></p>
		<p class="c1"><span class="c2"></span></p>
		</div>
    </body>
</html>

